{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAK9DredaNxSEx9lJqE4vA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ussef11/Certification_MERN/blob/main/NLP_SVM(Discriminative).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ceAD_Syv5h-d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from pathlib import Path\n",
        "from datetime import datetime as dt\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import os\n",
        "from glob import glob\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njJ4kdfwDU_Q",
        "outputId": "8223fd40-c2eb-4354-9aea-4b0397ff9c95"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_bio_file(path, encoding=\"utf-8\"):\n",
        "    sentences = []\n",
        "    sent = []\n",
        "\n",
        "    with open(path, \"r\", encoding=encoding) as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            # blank line = sentence boundary\n",
        "            if not line:\n",
        "                if sent:\n",
        "                    sentences.append(sent)\n",
        "                    sent = []\n",
        "                continue\n",
        "\n",
        "            # expect: \"token TAG\"\n",
        "            parts = line.split(\" \")\n",
        "            if len(parts) < 2:\n",
        "                # skip malformed lines (rare)\n",
        "                continue\n",
        "\n",
        "            token = parts[0]\n",
        "            tag = parts[-1]  # in case there are extra spaces, keep last as tag\n",
        "            sent.append((token, tag))\n",
        "\n",
        "    # last sentence if file doesn't end with blank line\n",
        "    if sent:\n",
        "        sentences.append(sent)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "def read_bio_folder(folder_path):\n",
        "    all_sentences = []\n",
        "    all_files = sorted(glob(os.path.join(folder_path, \"**\", \"*.txt\"), recursive=True))\n",
        "\n",
        "    for fp in all_files:\n",
        "        sents = read_bio_file(fp)\n",
        "        all_sentences.extend(sents)\n",
        "\n",
        "    return all_sentences, all_files\n"
      ],
      "metadata": {
        "id": "aG4Z0G2gDscc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/AQMAR_Arabic_NER_corpus-1.0'\n",
        "\n",
        "sents, file_list = read_bio_folder(DATA_PATH)\n",
        "\n",
        "print(\"Number of files:\", len(file_list))\n",
        "print(\"Number of sentences:\", len(sents))\n",
        "print(\"First sentence length:\", len(sents[0]))\n",
        "\n",
        "print(\"First 10 tokens/tags in first sentence:\")\n",
        "for tok, tag in sents[0][:10]:\n",
        "    print(tok, tag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyhhj0bYD3nr",
        "outputId": "d8b81d69-79d0-4eae-e6b6-c60faa00b062"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files: 40\n",
            "Number of sentences: 2689\n",
            "First sentence length: 14\n",
            "First 10 tokens/tags in first sentence:\n",
            "الذرة O\n",
            "هي O\n",
            "أصغر O\n",
            "جزء O\n",
            "من O\n",
            "العنصر O\n",
            "الكيميائي O\n",
            "الذي O\n",
            "يحتفظ O\n",
            "بالخصائص O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [[tok for tok, tag in sent] for sent in sents]\n",
        "tags   = [[tag for tok, tag in sent] for sent in sents]\n",
        "\n",
        "print(tokens[0][:10])\n",
        "print(tags[0][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcpcY6JmNz41",
        "outputId": "d4117a8b-d14a-4022-c0fb-b916567915b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['الذرة', 'هي', 'أصغر', 'جزء', 'من', 'العنصر', 'الكيميائي', 'الذي', 'يحتفظ', 'بالخصائص']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_tags(sents):\n",
        "    fixed = []\n",
        "    for sent in sents:\n",
        "        new_sent = []\n",
        "        for tok, tag in sent:\n",
        "            if tag == \"0\":\n",
        "                tag = \"O\"\n",
        "            new_sent.append((tok, tag))\n",
        "        fixed.append(new_sent)\n",
        "    return fixed\n",
        "\n",
        "sents = normalize_tags(sents)\n"
      ],
      "metadata": {
        "id": "sNUKBBmzNlhV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [[tok for tok, tag in sent] for sent in sents]\n",
        "tags   = [[tag for tok, tag in sent] for sent in sents]\n",
        "\n",
        "\n",
        "print(tokens[0][:10])\n",
        "print(tags[0][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D3DNOgANrCc",
        "outputId": "8e15c33a-59a9-4b3a-8bc7-7ea579265daa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['الذرة', 'هي', 'أصغر', 'جزء', 'من', 'العنصر', 'الكيميائي', 'الذي', 'يحتفظ', 'بالخصائص']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def token_features(sent, i):\n",
        "    w = sent[i][0]\n",
        "    feats = {\n",
        "        \"w\": w,\n",
        "        \"w_len\": len(w),\n",
        "        \"is_digit\": w.isdigit(),\n",
        "        \"pref2\": w[:2],\n",
        "        \"pref3\": w[:3],\n",
        "        \"suf2\": w[-2:],\n",
        "        \"suf3\": w[-3:],\n",
        "    }\n",
        "\n",
        "    # previous word\n",
        "    if i > 0:\n",
        "        w_prev = sent[i-1][0]\n",
        "        feats.update({\n",
        "            \"prev_w\": w_prev,\n",
        "            \"prev_pref2\": w_prev[:2],\n",
        "            \"prev_suf2\": w_prev[-2:],\n",
        "        })\n",
        "    else:\n",
        "        feats[\"BOS\"] = True\n",
        "\n",
        "    # next word\n",
        "    if i < len(sent) - 1:\n",
        "        w_next = sent[i+1][0]\n",
        "        feats.update({\n",
        "            \"next_w\": w_next,\n",
        "            \"next_pref2\": w_next[:2],\n",
        "            \"next_suf2\": w_next[-2:],\n",
        "        })\n",
        "    else:\n",
        "        feats[\"EOS\"] = True\n",
        "\n",
        "    return feats\n",
        "\n",
        "\n",
        "test = token_features(sents[0], 0)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYFW-b77N6Ju",
        "outputId": "a767983c-b17d-4d3c-c82b-a75973ba7c2b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'w': 'الذرة', 'w_len': 5, 'is_digit': False, 'pref2': 'ال', 'pref3': 'الذ', 'suf2': 'رة', 'suf3': 'ذرة', 'BOS': True, 'next_w': 'هي', 'next_pref2': 'هي', 'next_suf2': 'هي'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_feats, y_labels, sent_ids = [], [], []\n",
        "\n",
        "for sid, sent in enumerate(sents):\n",
        "    for i in range(len(sent)):\n",
        "        X_feats.append(token_features(sent, i))\n",
        "        y_labels.append(sent[i][1])\n",
        "        sent_ids.append(sid)\n",
        "\n",
        "len(X_feats), len(y_labels), len(set(sent_ids))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBMsATZIP-qp",
        "outputId": "0a0eaab9-8984-4006-9ef3-ee86e24b07e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(143432, 143432, 2689)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "gss = GroupShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
        "train_idx, test_idx = next(gss.split(X_feats, y_labels, groups=sent_ids))\n",
        "\n",
        "X_train = [X_feats[i] for i in train_idx]\n",
        "y_train = [y_labels[i] for i in train_idx]\n",
        "\n",
        "X_test  = [X_feats[i] for i in test_idx]\n",
        "y_test  = [y_labels[i] for i in test_idx]\n",
        "\n",
        "len(X_train), len(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94XgKOhmQGCS",
        "outputId": "5863e526-80d7-48a7-97d2-8c3f5509f7bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(103597, 39835)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "svm_model = Pipeline([\n",
        "    (\"vec\", DictVectorizer(sparse=True)),\n",
        "    (\"clf\", LinearSVC())\n",
        "])\n",
        "\n",
        "svm_model.fit(X_train, y_train)\n",
        "y_pred = svm_model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "1W1v0nC3RVZg",
        "outputId": "bc445c18-db5d-448f-be29-9483b8cabdee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2843405294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m ])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msvm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ],
      "metadata": {
        "id": "sw930nHPR9Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def simple_ar_tokenize(text):\n",
        "    print(text)\n",
        "    text = re.sub(r\"([.,!?;:()\\[\\]{}\\\"'،؛؟])\", r\" \\1 \", text)\n",
        "    print(text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.split(\" \")\n",
        "\n",
        "def predict_sentence_svm(model, text):\n",
        "    tokens = simple_ar_tokenize(text)\n",
        "    sent = [(t, \"O\") for t in tokens]\n",
        "    X = [token_features(sent, i) for i in range(len(sent))]\n",
        "    y_pred = model.predict(X)\n",
        "    return tokens, list(y_pred)\n"
      ],
      "metadata": {
        "id": "Ia0b4uWYSDzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_text = \" والمسلمين القسطنطينية بدأت الحملات  الصَليبية .في القرن الحادي عشر أوروبا\"\n",
        "tokens, pred_tags = predict_sentence_svm(svm_model, test_text)\n",
        "\n",
        "for t, tag in zip(tokens, pred_tags):\n",
        "    print(t, tag)\n"
      ],
      "metadata": {
        "id": "wYdHTFz8SpVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}